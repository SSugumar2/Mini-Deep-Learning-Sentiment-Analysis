{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7969c187-a110-4753-99c7-e6da002a2682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fa2c496-e65b-4f85-91f5-b48c85848f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDb dataset (Only top 10,000 most common words will be kept)\n",
    "vocab_size = 10000\n",
    "maxlen = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43aac525-2d25-41e1-b452-a6bc7dc69a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDb dataset (Only top 10,000 most common words will be kept)\n",
    "vocab_size = 10000  # Vocabulary size\n",
    "maxlen = 200        # Maximum length of reviews\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Pad sequences to ensure uniform input length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632677cb-89c3-4ae9-a824-2a1dd01d856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=32, input_length=maxlen))\n",
    "\n",
    "# Flatten the output of the embedding layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# Add a fully connected layer with ReLU activation\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "\n",
    "# Output layer with a single neuron (sigmoid for binary classification)\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "# Compile the model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c9e966-78bd-4593-9014-1cecd1d188a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 6s 14ms/step - loss: 0.4501 - accuracy: 0.7665 - val_loss: 0.3110 - val_accuracy: 0.8698\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.1273 - accuracy: 0.9548 - val_loss: 0.3600 - val_accuracy: 0.8598\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0206 - accuracy: 0.9963 - val_loss: 0.4348 - val_accuracy: 0.8572\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0029 - accuracy: 0.9999 - val_loss: 0.4505 - val_accuracy: 0.8670\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4749 - val_accuracy: 0.8678\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 6.0032e-04 - accuracy: 1.0000 - val_loss: 0.4979 - val_accuracy: 0.8668\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 3.8768e-04 - accuracy: 1.0000 - val_loss: 0.5151 - val_accuracy: 0.8690\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 2.6934e-04 - accuracy: 1.0000 - val_loss: 0.5315 - val_accuracy: 0.8690\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.9416e-04 - accuracy: 1.0000 - val_loss: 0.5451 - val_accuracy: 0.8698\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.4520e-04 - accuracy: 1.0000 - val_loss: 0.5591 - val_accuracy: 0.8686\n"
     ]
    }
   ],
   "source": [
    "# Initialize CSVLogger to save training logs\n",
    "csv_logger = CSVLogger('training_log.csv', append = True)\n",
    "\n",
    "# Train the model and save training logs\n",
    "history = model.fit(x_train, y_train, epochs = 10, batch_size = 64, validation_split = 0.2, callbacks =[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b517b070-62ca-41c7-88af-8ffec23beb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5750 - accuracy: 0.8628\n",
      "Test Accuracy: 0.8628\n",
      "Test Loss: 0.5750\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9063d112-accf-4440-bed6-6a5482ef9a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Accuracy ---\n",
      "Max Training Accuracy: 1.0000\n",
      "Min Training Accuracy: 0.7665\n",
      "Average Training Accuracy: 0.9717\n",
      "Final Epoch Training Accuracy: 1.0000\n",
      "\n",
      "--- Validation Accuracy ---\n",
      "Max Validation Accuracy: 0.8698\n",
      "Min Validation Accuracy: 0.8572\n",
      "Average Validation Accuracy: 0.8665\n",
      "Final Epoch Validation Accuracy: 0.8686\n",
      "\n",
      "--- Training Loss ---\n",
      "Max Training Loss: 0.4501\n",
      "Min Training Loss: 0.0001\n",
      "Average Training Loss: 0.0604\n",
      "Final Epoch Training Loss: 0.0001\n",
      "\n",
      "--- Validation Loss ---\n",
      "Max Validation Loss: 0.5591\n",
      "Min Validation Loss: 0.3110\n",
      "Average Validation Loss: 0.4680\n",
      "Final Epoch Validation Loss: 0.5591\n"
     ]
    }
   ],
   "source": [
    "#interpret the values using non-visual techniques\n",
    "import numpy as np\n",
    "\n",
    "# Calculate summary statistics for accuracy and loss\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "def summarize_metrics(metric_values, metric_name):\n",
    "    print(f\"\\n--- {metric_name} ---\")\n",
    "    print(f\"Max {metric_name}: {np.max(metric_values):.4f}\")\n",
    "    print(f\"Min {metric_name}: {np.min(metric_values):.4f}\")\n",
    "    print(f\"Average {metric_name}: {np.mean(metric_values):.4f}\")\n",
    "    print(f\"Final Epoch {metric_name}: {metric_values[-1]:.4f}\")\n",
    "\n",
    "# Summarize accuracy and loss metrics\n",
    "summarize_metrics(train_accuracy, \"Training Accuracy\")\n",
    "summarize_metrics(val_accuracy, \"Validation Accuracy\")\n",
    "summarize_metrics(train_loss, \"Training Loss\")\n",
    "summarize_metrics(val_loss, \"Validation Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13ca1fb-c8ef-4c84-b9fd-0d066da21922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
